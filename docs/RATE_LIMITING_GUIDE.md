# Rate Limiting Guide - go-deep-agent

**Status:** ğŸš§ Not Implemented (Planned for v0.8.0)  
**Priority:** P1 (High)  
**Complexity:** Medium  
**Estimated Time:** 1-2 weeks

---

## ğŸ“š Má»¥c lá»¥c

1. [Rate Limiting lÃ  gÃ¬?](#rate-limiting-lÃ -gÃ¬)
2. [Táº¡i sao cáº§n Rate Limiting?](#táº¡i-sao-cáº§n-rate-limiting)
3. [CÃ¡c loáº¡i Rate Limiting](#cÃ¡c-loáº¡i-rate-limiting)
4. [Thiáº¿t káº¿ Ä‘á» xuáº¥t](#thiáº¿t-káº¿-Ä‘á»-xuáº¥t)
5. [VÃ­ dá»¥ triá»ƒn khai](#vÃ­-dá»¥-triá»ƒn-khai)
6. [Best Practices](#best-practices)

---

## Rate Limiting lÃ  gÃ¬?

**Rate Limiting** lÃ  ká»¹ thuáº­t giá»›i háº¡n sá»‘ lÆ°á»£ng requests mÃ  má»™t client cÃ³ thá»ƒ thá»±c hiá»‡n trong má»™t khoáº£ng thá»i gian nháº¥t Ä‘á»‹nh.

### VÃ­ dá»¥ Ä‘Æ¡n giáº£n:

```
Giá»›i háº¡n: 100 requests/phÃºt
- Request 1-100: âœ… Cho phÃ©p
- Request 101:   âŒ Tá»« chá»‘i vá»›i lá»—i "Rate limit exceeded"
- Sau 1 phÃºt:    âœ… Counter reset vá» 0, láº¡i cho phÃ©p 100 requests má»›i
```

### Thuáº­t ngá»¯ quan trá»ng:

- **Rate**: Tá»‘c Ä‘á»™ cho phÃ©p (vÃ­ dá»¥: 100 requests/minute)
- **Burst**: Sá»‘ lÆ°á»£ng requests tá»‘i Ä‘a trong thá»i Ä‘iá»ƒm ngáº¯n (vÃ­ dá»¥: 10 requests/second)
- **Window**: Khoáº£ng thá»i gian tÃ­nh toÃ¡n (sliding window, fixed window, token bucket)
- **Quota**: Tá»•ng sá»‘ lÆ°á»£ng tÃ i nguyÃªn Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng (vÃ­ dá»¥: 1M tokens/month)

---

## Táº¡i sao cáº§n Rate Limiting?

### ğŸ›¡ï¸ 1. Báº£o vá»‡ khá»i láº¡m dá»¥ng (Abuse Protection)

**Váº¥n Ä‘á»:**
```go
// Káº» táº¥n cÃ´ng cÃ³ thá»ƒ gá»­i vÃ´ sá»‘ requests
for i := 0; i < 1000000; i++ {
    ai.Ask(ctx, "Spam request")  // KhÃ´ng bá»‹ giá»›i háº¡n!
}
```

**Háº­u quáº£:**
- Chi phÃ­ API tÄƒng vá»t (OpenAI tÃ­nh phÃ­ theo token)
- Server quÃ¡ táº£i
- Service bá»‹ cháº­m cho users khÃ¡c
- CÃ³ thá»ƒ bá»‹ OpenAI ban account

**Giáº£i phÃ¡p vá»›i Rate Limiting:**
```go
// Vá»›i rate limiting: Chá»‰ cho phÃ©p 100 req/phÃºt
for i := 0; i < 1000000; i++ {
    err := ai.Ask(ctx, "Request")
    if err == agent.ErrRateLimitExceeded {
        // Request 101+ bá»‹ tá»« chá»‘i
        time.Sleep(1 * time.Minute)  // Pháº£i Ä‘á»£i
    }
}
```

### ğŸ’° 2. Kiá»ƒm soÃ¡t chi phÃ­ (Cost Control)

**TÃ¬nh huá»‘ng thá»±c táº¿:**

```
OpenAI Pricing (GPT-4):
- Input:  $0.03 / 1K tokens
- Output: $0.06 / 1K tokens

KhÃ´ng cÃ³ rate limiting:
- User A gá»­i 10,000 requests (bug trong code)
- Má»—i request: 500 tokens input + 500 tokens output = 1,000 tokens
- Tá»•ng: 10,000 * 1,000 = 10M tokens
- Chi phÃ­: 10M / 1000 * $0.045 = $450 trong 1 giá»!
```

**Vá»›i rate limiting:**
```
Giá»›i háº¡n: 1,000 requests/hour per user
- User A bá»‹ block sau 1,000 requests
- Chi phÃ­ tá»‘i Ä‘a: $45/hour thay vÃ¬ $450/hour
- Tiáº¿t kiá»‡m: 90%!
```

### âš–ï¸ 3. PhÃ¢n bá»• tÃ i nguyÃªn cÃ´ng báº±ng (Fair Usage)

**Ká»‹ch báº£n Multi-Tenant SaaS:**

```
Há»‡ thá»‘ng cÃ³ 100 users:
- KhÃ´ng rate limiting: User A gá»­i 90% traffic â†’ 99 users khÃ¡c bá»‹ cháº­m
- CÃ³ rate limiting: Má»—i user tá»‘i Ä‘a 100 req/min â†’ Fair cho táº¥t cáº£
```

### ğŸ”’ 4. TuÃ¢n thá»§ API Provider Limits

**OpenAI API Limits:**
```
Tier 1 (Free):
- 3 requests/minute
- 200 requests/day
- 40,000 tokens/day

Tier 5 (Enterprise):
- 10,000 requests/minute
- 2,000,000 tokens/minute
```

**Náº¿u khÃ´ng cÃ³ rate limiting:**
```go
// Code nÃ y sáº½ bá»‹ OpenAI reject
for i := 0; i < 100; i++ {
    ai.Ask(ctx, "Question")  // Request 4+ â†’ 429 Error
}
```

### ğŸš¨ 5. PhÃ²ng chá»‘ng DoS/DDoS

**Denial of Service Attack:**
```
Attacker gá»­i 1 triá»‡u requests/giÃ¢y
â†’ Há»‡ thá»‘ng quÃ¡ táº£i
â†’ Service down cho users há»£p lá»‡
â†’ Doanh thu bá»‹ máº¥t
```

**Vá»›i rate limiting:**
```
Má»—i IP chá»‰ cho phÃ©p 100 req/min
â†’ Attacker bá»‹ block sau 100 requests
â†’ Service váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
```

---

## CÃ¡c loáº¡i Rate Limiting

### 1. **Fixed Window** (Cá»­a sá»• cá»‘ Ä‘á»‹nh)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
```
Window: 1 phÃºt
Limit: 100 requests

Minute 1 (00:00-00:59):
- Request 1-100:  âœ… Allowed
- Request 101+:   âŒ Blocked

Minute 2 (01:00-01:59):
- Counter reset vá» 0
- Request 1-100:  âœ… Allowed again
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, dá»… implement
- âœ… Hiá»‡u nÄƒng cao (chá»‰ cáº§n 1 counter)
- âœ… Dá»… hiá»ƒu vá»›i users

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ CÃ³ thá»ƒ bá»‹ burst at window boundary
  ```
  00:59 â†’ 50 requests
  01:00 â†’ 50 requests
  = 100 requests trong 1 giÃ¢y!
  ```

**Use case:**
- API Ä‘Æ¡n giáº£n
- KhÃ´ng quan trá»ng viá»‡c burst ngáº¯n háº¡n

### 2. **Sliding Window** (Cá»­a sá»• trÆ°á»£t)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
```
Limit: 100 requests/phÃºt
Current time: 12:00:30

Count requests trong 60 giÃ¢y trÆ°á»›c:
- Tá»« 11:59:30 Ä‘áº¿n 12:00:30
- Náº¿u < 100: Allow
- Náº¿u >= 100: Block

12:00:31 â†’ Window trÆ°á»£t: 11:59:31 - 12:00:31
12:00:32 â†’ Window trÆ°á»£t: 11:59:32 - 12:00:32
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ChÃ­nh xÃ¡c hÆ¡n Fixed Window
- âœ… NgÄƒn cháº·n burst at boundary
- âœ… PhÃ¢n phá»‘i Ä‘á»u traffic

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Phá»©c táº¡p hÆ¡n (cáº§n lÆ°u timestamp má»—i request)
- âŒ Tá»‘n memory (lÆ°u lá»‹ch sá»­ requests)
- âŒ Hiá»‡u nÄƒng tháº¥p hÆ¡n Fixed Window

**Use case:**
- API quan trá»ng cáº§n chÃ­nh xÃ¡c
- Multi-tenant SaaS

### 3. **Token Bucket** (ThÃ¹ng token) - â­ KhuyÃªn dÃ¹ng

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
Bucket capacity: 100 tokens
Refill rate: 10 tokens/giÃ¢y

Tráº¡ng thÃ¡i ban Ä‘áº§u: 100 tokens

Request 1: Consume 1 token â†’ 99 tokens cÃ²n láº¡i âœ…
Request 2: Consume 1 token â†’ 98 tokens âœ…
...
Request 101: No tokens left â†’ âŒ BLOCKED

Sau 1 giÃ¢y: +10 tokens â†’ 10 tokens
Request 102-111: Consume 10 tokens â†’ âœ… Allowed
Request 112: No tokens â†’ âŒ BLOCKED
```

**CÃ´ng thá»©c:**
```go
// Cáº­p nháº­t sá»‘ tokens
tokensToAdd = (currentTime - lastRefill) * refillRate
currentTokens = min(currentTokens + tokensToAdd, bucketCapacity)

// Kiá»ƒm tra request
if currentTokens >= requestCost {
    currentTokens -= requestCost
    return ALLOW
} else {
    return BLOCK
}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Cho phÃ©p burst ngáº¯n háº¡n (khi bucket Ä‘áº§y)
- âœ… Smooth traffic over time
- âœ… Hiá»‡u nÄƒng tá»‘t (chá»‰ cáº§n 2 variables: tokens, lastRefill)
- âœ… Linh hoáº¡t (cÃ³ thá»ƒ set burst capacity)

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ HÆ¡i phá»©c táº¡p Ä‘á»ƒ hiá»ƒu
- âŒ Cáº§n tÃ­nh toÃ¡n refill

**Use case:** â­ **KhuyÃªn dÃ¹ng cho go-deep-agent**
- CÃ¢n báº±ng giá»¯a hiá»‡u nÄƒng vÃ  chÃ­nh xÃ¡c
- Cho phÃ©p burst há»£p lÃ½
- PhÃ¹ há»£p vá»›i LLM API (cÃ³ peak traffic)

### 4. **Leaky Bucket** (ThÃ¹ng dÃ²)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
Queue capacity: 100 requests
Processing rate: 10 requests/giÃ¢y

Request Ä‘áº¿n â†’ VÃ o queue
Queue â†’ Process vá»›i tá»‘c Ä‘á»™ cá»‘ Ä‘á»‹nh

Queue: [R1, R2, R3, ...]
       â†“ 10 req/s
     Process
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Traffic smoothing (output rate Ä‘á»u)
- âœ… Báº£o vá»‡ downstream services

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Latency cao (pháº£i queue)
- âŒ KhÃ´ng phÃ¹ há»£p real-time

**Use case:**
- Message queue systems
- Batch processing

---

## Thiáº¿t káº¿ Ä‘á» xuáº¥t cho go-deep-agent

### Architecture Overview

```
User Request
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rate Limiter       â”‚
â”‚  (Token Bucket)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (if allowed)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent.Ask()        â”‚
â”‚  Agent.Stream()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI API         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Design

```go
package agent

// RateLimiter interface
type RateLimiter interface {
    // Allow checks if request is allowed
    Allow(ctx context.Context, key string) (bool, error)
    
    // Wait waits until request is allowed (blocking)
    Wait(ctx context.Context, key string) error
    
    // Reserve reserves permission for future use
    Reserve(ctx context.Context, key string) (*Reservation, error)
    
    // Stats returns current rate limit statistics
    Stats(ctx context.Context, key string) (*RateLimitStats, error)
}

// RateLimitConfig configuration
type RateLimitConfig struct {
    // Strategy: "token-bucket", "fixed-window", "sliding-window"
    Strategy string
    
    // Rate: requests per second
    Rate int
    
    // Burst: maximum burst size (for token bucket)
    Burst int
    
    // KeyFunc: function to extract rate limit key from context
    // Default: per IP, can be per user, per API key, etc.
    KeyFunc func(ctx context.Context) string
    
    // OnRateLimitExceeded: callback when limit exceeded
    OnRateLimitExceeded func(ctx context.Context, key string)
    
    // Storage: where to store counters (memory, redis)
    Storage RateLimitStorage
}

// RateLimitStats statistics
type RateLimitStats struct {
    Key           string
    Limit         int
    Remaining     int
    ResetAt       time.Time
    RetryAfter    time.Duration
}

// Builder methods
func (b *Builder) WithRateLimit(config *RateLimitConfig) *Builder
func (b *Builder) WithRateLimitPerSecond(rate, burst int) *Builder
func (b *Builder) WithRateLimitPerMinute(rate, burst int) *Builder
func (b *Builder) WithRateLimitPerHour(rate, burst int) *Builder
```

### Implementation vá»›i Token Bucket

```go
package agent

import (
    "context"
    "sync"
    "time"
    "golang.org/x/time/rate"
)

// TokenBucketLimiter implementation
type TokenBucketLimiter struct {
    limiters sync.Map // map[string]*rate.Limiter
    rate     rate.Limit
    burst    int
    mu       sync.RWMutex
}

func NewTokenBucketLimiter(rps int, burst int) *TokenBucketLimiter {
    return &TokenBucketLimiter{
        rate:  rate.Limit(rps),
        burst: burst,
    }
}

func (l *TokenBucketLimiter) getLimiter(key string) *rate.Limiter {
    limiter, exists := l.limiters.Load(key)
    if !exists {
        limiter = rate.NewLimiter(l.rate, l.burst)
        l.limiters.Store(key, limiter)
    }
    return limiter.(*rate.Limiter)
}

func (l *TokenBucketLimiter) Allow(ctx context.Context, key string) (bool, error) {
    limiter := l.getLimiter(key)
    return limiter.Allow(), nil
}

func (l *TokenBucketLimiter) Wait(ctx context.Context, key string) error {
    limiter := l.getLimiter(key)
    return limiter.Wait(ctx)
}

func (l *TokenBucketLimiter) Reserve(ctx context.Context, key string) (*Reservation, error) {
    limiter := l.getLimiter(key)
    r := limiter.Reserve()
    
    if !r.OK() {
        return nil, ErrRateLimitExceeded
    }
    
    return &Reservation{
        reservation: r,
        delay:       r.Delay(),
    }, nil
}

func (l *TokenBucketLimiter) Stats(ctx context.Context, key string) (*RateLimitStats, error) {
    limiter := l.getLimiter(key)
    
    // Calculate remaining tokens
    r := limiter.Reserve()
    defer r.Cancel()
    
    remaining := l.burst
    if !r.OK() {
        remaining = 0
    }
    
    return &RateLimitStats{
        Key:        key,
        Limit:      l.burst,
        Remaining:  remaining,
        ResetAt:    time.Now().Add(r.Delay()),
        RetryAfter: r.Delay(),
    }, nil
}
```

### Redis-backed Rate Limiter (Distributed)

```go
package agent

import (
    "context"
    "time"
    "github.com/redis/go-redis/v9"
)

// RedisRateLimiter for distributed rate limiting
type RedisRateLimiter struct {
    client *redis.Client
    rate   int
    window time.Duration
}

func NewRedisRateLimiter(client *redis.Client, rate int, window time.Duration) *RedisRateLimiter {
    return &RedisRateLimiter{
        client: client,
        rate:   rate,
        window: window,
    }
}

func (l *RedisRateLimiter) Allow(ctx context.Context, key string) (bool, error) {
    // Sliding window with Redis
    now := time.Now()
    windowStart := now.Add(-l.window)
    
    pipe := l.client.Pipeline()
    
    // Remove old entries
    pipe.ZRemRangeByScore(ctx, key, "0", fmt.Sprintf("%d", windowStart.UnixNano()))
    
    // Count entries in window
    countCmd := pipe.ZCard(ctx, key)
    
    // Add current request
    pipe.ZAdd(ctx, key, redis.Z{
        Score:  float64(now.UnixNano()),
        Member: now.UnixNano(),
    })
    
    // Set expiry
    pipe.Expire(ctx, key, l.window)
    
    _, err := pipe.Exec(ctx)
    if err != nil {
        return false, err
    }
    
    count := countCmd.Val()
    return count < int64(l.rate), nil
}
```

---

## VÃ­ dá»¥ triá»ƒn khai

### Example 1: Basic Rate Limiting

```go
package main

import (
    "context"
    "fmt"
    "time"
    "github.com/taipm/go-deep-agent/agent"
)

func main() {
    ctx := context.Background()
    
    // Create agent with rate limiting
    ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimitPerMinute(100, 10).  // 100 req/min, burst 10
        WithAutoExecute(true)
    
    // Normal usage - first 10 requests are fast (burst)
    for i := 0; i < 15; i++ {
        start := time.Now()
        
        resp, err := ai.Ask(ctx, "Hello")
        if err != nil {
            if err == agent.ErrRateLimitExceeded {
                fmt.Printf("Request %d: Rate limited!\n", i+1)
                continue
            }
            panic(err)
        }
        
        fmt.Printf("Request %d: %s (took %v)\n", i+1, resp, time.Since(start))
    }
    
    // Output:
    // Request 1: Hello! (took 500ms)   - From burst
    // Request 2: Hello! (took 501ms)   - From burst
    // ...
    // Request 10: Hello! (took 502ms)  - From burst
    // Request 11: Rate limited!        - Burst exhausted, waiting for refill
    // Request 12: Hello! (took 1.5s)   - After refill
}
```

### Example 2: Per-User Rate Limiting

```go
package main

import (
    "context"
    "github.com/taipm/go-deep-agent/agent"
)

func main() {
    // Rate limit by user ID
    config := &agent.RateLimitConfig{
        Strategy: "token-bucket",
        Rate:     100,  // 100 req/min
        Burst:    10,
        KeyFunc: func(ctx context.Context) string {
            // Extract user ID from context
            userID := ctx.Value("user_id").(string)
            return fmt.Sprintf("user:%s", userID)
        },
        OnRateLimitExceeded: func(ctx context.Context, key string) {
            log.Printf("Rate limit exceeded for %s", key)
            // Send alert, update metrics, etc.
        },
    }
    
    ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimit(config)
    
    // Different users have separate limits
    ctx1 := context.WithValue(context.Background(), "user_id", "user123")
    ctx2 := context.WithValue(context.Background(), "user_id", "user456")
    
    ai.Ask(ctx1, "Question 1")  // User 123's quota
    ai.Ask(ctx2, "Question 2")  // User 456's quota (separate)
}
```

### Example 3: Graceful Degradation

```go
package main

import (
    "context"
    "time"
    "github.com/taipm/go-deep-agent/agent"
)

func main() {
    ctx := context.Background()
    
    ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimitPerSecond(10, 5)
    
    // Strategy 1: Wait for availability
    err := ai.WaitForRateLimit(ctx, "user123")
    if err == nil {
        resp, _ := ai.Ask(ctx, "Question")
        fmt.Println(resp)
    }
    
    // Strategy 2: Check before making request
    allowed, _ := ai.CheckRateLimit(ctx, "user123")
    if allowed {
        resp, _ := ai.Ask(ctx, "Question")
        fmt.Println(resp)
    } else {
        // Fallback to cached response
        resp := getCachedResponse("Question")
        fmt.Println("Cached:", resp)
    }
    
    // Strategy 3: Reserve slot
    reservation, err := ai.ReserveRateLimit(ctx, "user123")
    if err == nil {
        time.Sleep(reservation.Delay())  // Wait if needed
        resp, _ := ai.Ask(ctx, "Question")
        fmt.Println(resp)
    }
}
```

### Example 4: Multi-Tier Rate Limiting

```go
package main

import (
    "context"
    "github.com/taipm/go-deep-agent/agent"
)

func createAgentForTier(tier string, apiKey string) *agent.Builder {
    var rate, burst int
    
    switch tier {
    case "free":
        rate = 10    // 10 req/min
        burst = 2    // 2 burst
    case "basic":
        rate = 100   // 100 req/min
        burst = 10   // 10 burst
    case "premium":
        rate = 1000  // 1000 req/min
        burst = 100  // 100 burst
    case "enterprise":
        rate = 10000 // Unlimited (high limit)
        burst = 1000
    }
    
    return agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimitPerMinute(rate, burst)
}

func main() {
    ctx := context.Background()
    
    // Free tier user
    freeAI := createAgentForTier("free", apiKey)
    freeAI.Ask(ctx, "Question")  // Limited to 10/min
    
    // Premium tier user
    premiumAI := createAgentForTier("premium", apiKey)
    premiumAI.Ask(ctx, "Question")  // Can do 1000/min
}
```

### Example 5: Rate Limit with Metrics

```go
package main

import (
    "context"
    "github.com/taipm/go-deep-agent/agent"
    "github.com/prometheus/client_golang/prometheus"
)

var (
    rateLimitCounter = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "rate_limit_exceeded_total",
            Help: "Total number of rate limit exceeded errors",
        },
        []string{"user_id"},
    )
)

func main() {
    ctx := context.Background()
    
    config := &agent.RateLimitConfig{
        Strategy: "token-bucket",
        Rate:     100,
        Burst:    10,
        OnRateLimitExceeded: func(ctx context.Context, key string) {
            // Emit metric
            rateLimitCounter.WithLabelValues(key).Inc()
            
            // Log
            log.Printf("Rate limit exceeded: %s", key)
            
            // Alert (if threshold exceeded)
            if getExceededCount(key) > 100 {
                sendAlert("Possible abuse detected", key)
            }
        },
    }
    
    ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimit(config)
    
    // Usage
    _, err := ai.Ask(ctx, "Question")
    if err == agent.ErrRateLimitExceeded {
        // Handle rate limit
        stats, _ := ai.GetRateLimitStats(ctx)
        fmt.Printf("Retry after: %v\n", stats.RetryAfter)
    }
}
```

---

## Best Practices

### 1. Chá»n chiáº¿n lÆ°á»£c phÃ¹ há»£p

```
Fixed Window:
- âœ… Use: Simple APIs, non-critical applications
- âŒ Avoid: Paid APIs, multi-tenant systems

Sliding Window:
- âœ… Use: High-accuracy requirements
- âŒ Avoid: High-throughput systems (memory intensive)

Token Bucket: â­ RECOMMENDED
- âœ… Use: Most production scenarios
- âœ… Allows burst, smooth traffic
- âœ… Good performance

Leaky Bucket:
- âœ… Use: When need strict output rate
- âŒ Avoid: Real-time applications (high latency)
```

### 2. Thiáº¿t láº­p limits há»£p lÃ½

```go
// âŒ BAD: QuÃ¡ restrictive
ai.WithRateLimitPerSecond(1, 1)  // Users sáº½ frustrate

// âŒ BAD: QuÃ¡ lá»ng láº»o
ai.WithRateLimitPerSecond(10000, 5000)  // KhÃ´ng cÃ³ tÃ¡c dá»¥ng

// âœ… GOOD: CÃ¢n báº±ng
ai.WithRateLimitPerMinute(100, 10)  // Reasonable cho most users
```

**CÃ¡ch tÃ­nh limits:**

```
OpenAI Tier 5 limit: 10,000 RPM
Expected users: 100
Safety margin: 20%

Per-user limit: 10,000 * 0.8 / 100 = 80 req/min
Burst: 10-20% of rate = 8-16 (chá»n 10)
```

### 3. Implement graceful degradation

```go
// âœ… GOOD: Fallback strategy
resp, err := ai.Ask(ctx, question)
if err == agent.ErrRateLimitExceeded {
    // Strategy 1: Wait and retry
    stats, _ := ai.GetRateLimitStats(ctx)
    time.Sleep(stats.RetryAfter)
    resp, err = ai.Ask(ctx, question)
}

if err == agent.ErrRateLimitExceeded {
    // Strategy 2: Use cache
    resp = getCachedResponse(question)
}

if resp == "" {
    // Strategy 3: Return error with helpful message
    return fmt.Errorf("service busy, retry after %v", stats.RetryAfter)
}
```

### 4. ThÃ´ng bÃ¡o rÃµ rÃ ng cho users

```go
// âœ… GOOD: Clear error message
if err == agent.ErrRateLimitExceeded {
    stats, _ := ai.GetRateLimitStats(ctx)
    
    return &APIResponse{
        Error: "Rate limit exceeded",
        Message: fmt.Sprintf(
            "You've used %d/%d requests. Please retry after %v",
            stats.Limit - stats.Remaining,
            stats.Limit,
            stats.RetryAfter,
        ),
        RetryAfter: stats.ResetAt,
    }
}
```

### 5. Monitor vÃ  alert

```go
// Track metrics
rateLimitHits.Inc()
rateLimitRemaining.Set(float64(stats.Remaining))

// Alert on high rate limit hits
if hitRate > 0.8 {
    alert("80% of users hitting rate limit - consider increasing")
}

// Alert on abuse
if userHitCount > 1000 {
    alert("Possible abuse detected for user: " + userID)
}
```

### 6. Testing

```go
func TestRateLimit(t *testing.T) {
    ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithRateLimitPerSecond(10, 5)
    
    // Test burst
    for i := 0; i < 5; i++ {
        _, err := ai.Ask(ctx, "Question")
        assert.NoError(t, err)  // Should succeed
    }
    
    // Test rate limit
    for i := 0; i < 10; i++ {
        _, err := ai.Ask(ctx, "Question")
        if err == agent.ErrRateLimitExceeded {
            t.Log("Rate limited as expected")
            return
        }
    }
    
    t.Error("Should have hit rate limit")
}
```

---

## Implementation Roadmap

### Phase 1: Core Implementation (Week 1)

- [ ] `RateLimiter` interface
- [ ] Token Bucket implementation (memory)
- [ ] Builder API methods
- [ ] Error types and codes
- [ ] Unit tests

### Phase 2: Advanced Features (Week 2)

- [ ] Redis-backed rate limiter
- [ ] Per-user, per-IP, per-API-key strategies
- [ ] Rate limit statistics
- [ ] Metrics integration
- [ ] Documentation

### Phase 3: Testing & Optimization

- [ ] Load testing
- [ ] Benchmark suite
- [ ] Examples
- [ ] Production deployment guide

---

## TÃ i liá»‡u tham kháº£o

1. **golang.org/x/time/rate** - Go standard rate limiting library
2. **NGINX Rate Limiting** - Best practices guide
3. **Stripe API Rate Limiting** - Real-world example
4. **Redis Rate Limiting Patterns** - Distributed rate limiting
5. **RFC 6585** - HTTP Status Code 429 (Too Many Requests)

---

## FAQ

**Q: Rate limiting khÃ¡c gÃ¬ vá»›i throttling?**

A: 
- **Rate Limiting**: Hard limit, block requests khi vÆ°á»£t quota
- **Throttling**: Slow down requests, váº«n process nhÆ°ng cháº­m hÆ¡n

**Q: NÃªn dÃ¹ng memory hay Redis?**

A:
- **Memory**: Single instance, Ä‘Æ¡n giáº£n, nhanh
- **Redis**: Multi-instance, distributed, persistent

**Q: Token bucket vs leaky bucket?**

A:
- **Token Bucket**: Allows burst, smooth over time (RECOMMENDED)
- **Leaky Bucket**: Strict output rate, higher latency

**Q: LÃ m sao test rate limiting?**

A: 
```go
// Use time.Sleep hoáº·c mock time
ai.WithRateLimitPerSecond(10, 5)
// Send 20 requests rapidly
// Expect: 5 succeed immediately, 5 succeed after delay, 10 fail
```

---

**Status:** Draft - Ready for implementation  
**Next Steps:** Create GitHub issue for v0.8.0  
**Owner:** TBD  
**Target Release:** v0.8.0
