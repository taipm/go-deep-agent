# HÆ°á»›ng dáº«n test Chatbot CLI vá»›i Ollama

## Chuáº©n bá»‹

### 1. Pull model qwen2.5:1.5b (recommended - nhanh & nháº¹)

```bash
# Pull model (láº§n Ä‘áº§u tiÃªn, ~900MB)
ollama pull qwen2.5:1.5b

# Hoáº·c pull llama3.2 (lá»›n hÆ¡n, ~2GB)
ollama pull llama3.2

# Kiá»ƒm tra models Ä‘Ã£ cÃ³
ollama list
```

### 2. Äáº£m báº£o Ollama Ä‘ang cháº¡y

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Test Ollama
curl http://localhost:11434/api/tags
```

## Cháº¡y Chatbot

### CÃ¡ch 1: DÃ¹ng script test tá»± Ä‘á»™ng

```bash
cd examples
./test_ollama_chatbot.sh
```

### CÃ¡ch 2: Cháº¡y trá»±c tiáº¿p

```bash
cd examples
go run chatbot_cli.go

# Khi Ä‘Æ°á»£c há»i:
# Your choice (1-5): 4            <- Chá»n qwen2.5:1.5b
# Enable streaming mode? (y/n): y  <- Báº­t streaming
# Enable conversation memory? (y/n): y <- Báº­t memory
```

### CÃ¡ch 3: Build vÃ  cháº¡y

```bash
cd examples
go build chatbot_cli.go
./chatbot_cli
```

## Test scenarios

### Test 1: Simple conversation
```
You: Hello, what is Go?
AI: [streaming response about Go programming language]

You: What are its main features?
AI: [response with memory of previous context]
```

### Test 2: Cache statistics
```
You: What is 2+2?
AI: 4

You: /stats
ğŸ“Š Cache Statistics:
  Hits:       0
  Misses:     1
  Size:       1 entries

You: What is 2+2?  # Same question
AI: 4

You: /stats
ğŸ“Š Cache Statistics:
  Hits:       1      <- Cache hit!
  Misses:     1
  Hit Rate:   50.00%
```

### Test 3: Commands
```
You: /help
ğŸ“š Available Commands:
  /help   - Show this help message
  /clear  - Clear cache
  /stats  - Show cache statistics
  /exit   - Exit the chatbot

You: /clear
âœ… Cache cleared

You: /exit
ğŸ‘‹ Goodbye!
```

## Troubleshooting

### Error: model not found
```bash
# Pull the model
ollama pull qwen2.5:1.5b
```

### Error: connection refused
```bash
# Start Ollama service
ollama serve
```

### Slow responses
- qwen2.5:1.5b is fastest (~500ms typical)
- llama3.2 is slower but better quality (~1-2s)
- First request is slower (model loading)
- Subsequent requests are faster (model cached)

## Model comparison

| Model | Size | Speed | Quality | Memory |
|-------|------|-------|---------|--------|
| qwen2.5:1.5b | ~900MB | âš¡âš¡âš¡âš¡ | â­â­â­ | ~2GB RAM |
| llama3.2 | ~2GB | âš¡âš¡âš¡ | â­â­â­â­ | ~4GB RAM |

**Recommendation**: Start with qwen2.5:1.5b for fast, local testing!
