# Go Deep Agent üöÄ

A powerful yet simple LLM agent library for Go with a modern **Fluent Builder API**. Build AI applications with method chaining, automatic conversation memory, intelligent error handling, and seamless streaming support.

Built with [openai-go v3.8.1](https://github.com/openai/openai-go).

> **Why go-deep-agent?** 60-80% less code than openai-go with 10x better developer experience. [See detailed comparison ‚Üí](docs/COMPARISON.md)

## ‚ú® Features

- üéØ **Fluent Builder API** - Natural, readable method chaining
- ‚ö° **WithDefaults()** - Production-ready in one line: Memory(20) + Retry(3) + Timeout(30s) + ExponentialBackoff (v0.5.8 üÜï)
- ü§ñ **Multi-Provider** - OpenAI, Ollama, and custom endpoints
- üß† **Hierarchical Memory** - 3-tier system (Working ‚Üí Episodic ‚Üí Semantic) with automatic importance scoring (v0.6.0 üÜï)
- ÔøΩ **Session Persistence** - Save/restore conversations across executions with file-based or custom backends (v0.8.0 üÜï)
- ÔøΩüì° **Streaming** - Real-time response streaming with callbacks
- üõ†Ô∏è **Tool Calling** - Auto-execution with type-safe function definitions
- üìã **Structured Outputs** - JSON Schema with strict mode
- ‚ö° **Error Recovery** - Smart retries with exponential backoff
- üéõÔ∏è **Advanced Controls** - Temperature, top-p, tokens, penalties, seed
- üß™ **Production Ready** - Timeouts, retries, comprehensive error handling
- üñºÔ∏è **Multimodal** - Vision support for GPT-4 Vision (images via URL/file/base64)
- üöÄ **Batch Processing** - Concurrent request processing with progress tracking (v0.4.0)
- üìö **RAG Support** - Retrieval-Augmented Generation with document chunking (v0.4.0)
- üíæ **Response Caching** - Memory & Redis caching with TTL management (v0.4.0, v0.5.1 üÜï)
- üî¢ **Vector Embeddings** - OpenAI & Ollama embeddings with similarity search (v0.5.0 üÜï)
- üóÑÔ∏è **Vector Databases** - ChromaDB & Qdrant integration for semantic search (v0.5.0 üÜï)
- üß† **Vector RAG** - Semantic retrieval with auto-embedding and priority system (v0.5.0 üÜï)
- üìä **Logging & Observability** - Zero-overhead logging with slog support (v0.5.2 üÜï)
- üõ†Ô∏è **Built-in Tools** - FileSystem, HTTP, DateTime, Math tools (v0.5.5 üÜï convenient loading)
- üîç **Tools Logging** - Comprehensive logging for built-in tools with security auditing (v0.5.6 üÜï)
- üéì **Few-Shot Learning** - Teach agents with examples (inline or YAML personas) (v0.6.5 üÜï)
- ü§î **ReAct Pattern** - Native function calling + text parsing modes for autonomous multi-step reasoning (v0.7.5 üÜï)
- üß© **Planning Layer** - Goal decomposition, parallel execution, adaptive strategies for complex workflows (v0.7.1 üÜï)
- üö¶ **Rate Limiting** - Token bucket algorithm with per-key limits and burst capacity (v0.7.3 üÜï)
- ‚úÖ **Well Tested** - 1344+ tests, 71%+ coverage, 77+ working examples

## üì¶ Installation

```bash
go get github.com/taipm/go-deep-agent
```

## üöÄ Quick Start

### Simple Chat - One Line

```go
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).Ask(ctx, "What is Go?")
```

### With Streaming

```go
agent.NewOpenAI("gpt-4o-mini", apiKey).
    OnStream(func(content string) {
        fmt.Print(content)
    }).
    Stream(ctx, "Write a haiku about code")
```

### With Conversation Memory

```go
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).WithMemory()

builder.Ask(ctx, "My name is John")
builder.Ask(ctx, "What's my name?")  // AI remembers: "Your name is John"
```

### With Persistent Memory (v0.9.0+)

**Save conversations across program executions** - memories are automatically saved and restored:

#### File-Based (Built-in, Zero Config)

```go
// First conversation
agent := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().                    // Working memory (RAM)
    WithLongMemory("user-alice")          // Persistent memory (disk)

agent.Ask(ctx, "My favorite color is blue")
// Automatically saved to ~/.go-deep-agent/memories/user-alice.json

// Later (new program execution)
agent2 := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().
    WithLongMemory("user-alice")          // Auto-loads previous conversation

agent2.Ask(ctx, "What's my favorite color?")  // AI remembers: "Blue"
```

#### Redis Backend (v0.10.0+ - Production)

**For production deployments with shared memory across instances:**

```go
// ‚úÖ RECOMMENDED: Simple setup
backend := agent.NewRedisBackend("localhost:6379")
defer backend.Close()

agent := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().
    WithLongMemory("user-alice").
    UsingBackend(backend)

// Conversations stored in Redis with 7-day TTL (auto-extends on activity)
agent.Ask(ctx, "My favorite color is blue")
```

**With authentication:**

```go
backend := agent.NewRedisBackend("localhost:6379").
    WithPassword("your-redis-password")

agent := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().
    WithLongMemory("user-alice").
    UsingBackend(backend)
```

**Custom configuration:**

```go
backend := agent.NewRedisBackend("localhost:6379").
    WithPassword("secret").
    WithTTL(24 * time.Hour).      // Expire after 24h of inactivity
    WithPrefix("myapp:")           // Custom key prefix

agent := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().
    WithLongMemory("user-alice").
    UsingBackend(backend)
```

**Key Features:**
- üíæ **Auto-save** - Automatically persists after each `Ask()` or `Stream()`
- üîÑ **Auto-load** - Restores previous conversations on initialization
- üè™ **File-based** - Zero dependencies, works out-of-the-box (`~/.go-deep-agent/memories/`)
- üî¥ **Redis backend** - Production-ready with clustering, TTL, and connection pooling
- üîå **Pluggable** - Custom backends (PostgreSQL, S3, etc.)
- üîí **Thread-safe** - Concurrent access with atomic writes
- ‚èÆÔ∏è **Backward compatible** - Old API still works with deprecation warnings

**Manual Control:**

```go
agent := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithShortMemory().
    WithLongMemory("user-bob").
    WithAutoSaveLongMemory(false)  // Manual mode

// Manual operations
agent.SaveLongMemory(ctx)                  // Save explicitly
agent.LoadLongMemory(ctx)                  // Reload from storage
agent.DeleteLongMemory(ctx)                // Remove memory
memories, _ := agent.ListLongMemories(ctx) // List all memories
```

**üìñ Guides:**
- **[Redis Backend Guide](docs/REDIS_BACKEND_GUIDE.md)** - Installation, configuration, best practices (v0.10.1 updated)
- **[Memory System Guide](RELEASE_NOTES_v0.9.0.md)** - Memory terminology, migration from v0.8.0

### With Few-Shot Learning (v0.6.5 üÜï)

```go
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithSystem("You are a French translator.").
    AddFewShotExample("Translate: Hello", "Bonjour").
    AddFewShotExample("Translate: Goodbye", "Au revoir")

ai.Ask(ctx, "Translate: Good morning")  // AI follows the pattern
```

**[üìñ Few-Shot Learning Guide](docs/FEWSHOT_GUIDE.md)** - Selection modes, YAML personas, best practices

### With Production Defaults (v0.5.8 üÜï)

**The easiest way to get started** - one method call for production-ready configuration:

```go
// WithDefaults() gives you: Memory(20), Retry(3), Timeout(30s), ExponentialBackoff
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).WithDefaults()

// Now you're ready for production use with smart defaults
resp, _ := ai.Ask(ctx, "Hello!")
```

**Customize defaults via method chaining:**

```go
ai := agent.NewOpenAI("gpt-4", apiKey).
    WithDefaults().          // Start with smart defaults
    WithMaxHistory(50).      // Customize: Increase memory
    WithTools(myTool).       // Add: Tool capability
    WithLogging(logger)      // Add: Observability

resp, _ := ai.Ask(ctx, "Complex task...")
```

**Philosophy: Bare ‚Üí WithDefaults() ‚Üí Customize**

- **Bare**: `NewOpenAI(model, key)` - Full control, zero configuration
- **WithDefaults()**: Production-ready in one line (80% use cases)
- **Customize**: Progressive enhancement via method chaining

### Production-Ready Configuration

```go
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithSystem("You are a helpful assistant").
    WithTemperature(0.7).
    WithMaxTokens(500).
    WithMemory().
    WithMaxHistory(10).
    WithTimeout(30 * time.Second).
    WithRetry(3).
    WithExponentialBackoff().
    Ask(ctx, "Explain Go concurrency")
```

## üìñ Builder API Examples

### 1. OpenAI with System Prompt

```go
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithSystem("You are a helpful assistant").
    WithTemperature(0.7).
    Ask(ctx, "Explain quantum computing")
```

### 2. Streaming with Callbacks

```go
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    OnStream(func(content string) {
        fmt.Print(content)  // Print each chunk as it arrives
    }).
    Stream(ctx, "Write a haiku about AI")
```

### 3. Conversation Memory

```go
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithMemory().           // Enable automatic memory
    WithMaxHistory(10)      // Keep last 10 messages (auto-truncate)

// First message
builder.Ask(ctx, "My name is John and I'm from Vietnam")

// AI remembers previous context
builder.Ask(ctx, "What's my name and where am I from?")
// Response: "Your name is John and you're from Vietnam"
```

### 3.1 Hierarchical Memory (v0.6.0 üÜï)

**3-tier intelligent memory system**: Working ‚Üí Episodic ‚Üí Semantic

```go
// Automatic episodic storage for important messages
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithEpisodicMemory(0.7).           // Store messages with importance >= 0.7
    WithWorkingMemorySize(20).          // Working capacity
    WithSemanticMemory()                // Enable fact storage

// Important messages automatically stored in episodic memory
builder.Ask(ctx, "Remember: my birthday is Jan 15")  // importance: 1.0 ‚Üí episodic
builder.Ask(ctx, "How's the weather?")               // importance: 0.1 ‚Üí working only

// Recall from episodic memory
episodes := builder.GetMemory().Recall(ctx, "birthday", 5)

// Get detailed stats
stats := builder.GetMemory().Stats(ctx)
fmt.Printf("Working: %d, Episodic: %d, Semantic: %d\n", 
    stats.WorkingSize, stats.EpisodicSize, stats.SemanticSize)
```

**Importance weights** (customizable):
```go
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithImportanceWeights(agent.ImportanceWeights{
        RememberKeyword: 1.0,  // "Remember this", "Don't forget"
        PersonalInfo:    0.8,  // Names, dates, locations
        Question:        0.3,  // Questions from user
        Answer:          0.2,  // Answers from assistant
    })
```

üìö **See migration guide**: [docs/MEMORY_MIGRATION.md](docs/MEMORY_MIGRATION.md)

### 4. Tool Calling with Auto-Execution

```go
weatherTool := agent.NewTool("get_weather", "Get current weather").
    AddParameter("location", "string", "City name", true).
    WithHandler(func(args string) (string, error) {
        return `{"temp": 25, "condition": "sunny"}`, nil
    })

response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithTools(weatherTool).
    WithAutoExecute(true).  // Automatically execute tool calls
    Ask(ctx, "What's the weather in Hanoi?")
```

### 5. Structured Outputs (JSON Schema)

```go
schema := map[string]interface{}{
    "type": "object",
    "properties": map[string]interface{}{
        "name": map[string]interface{}{"type": "string"},
        "age":  map[string]interface{}{"type": "integer"},
    },
    "required":             []string{"name", "age"},
    "additionalProperties": false,
}

response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithJSONSchema("person_info", "Extract person info", schema, true).
    Ask(ctx, "John is 25 years old")
// Response: {"name":"John","age":25}
```

### 6. Error Handling with Retry

```go
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithTimeout(10 * time.Second).
    WithRetry(3).
    WithExponentialBackoff().  // 1s, 2s, 4s, 8s delays
    Ask(ctx, "What is Go?")

if err != nil {
    if agent.IsTimeoutError(err) {
        log.Println("Request timed out")
    } else if agent.IsRateLimitError(err) {
        log.Println("Rate limit exceeded")
    }
}
```

### 7. Rate Limiting (v0.7.3 üÜï)

**Control request rates to comply with API limits and manage costs:**

```go
// Simple rate limiting: 10 requests/second, burst of 20
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithRateLimit(10.0, 20).
    WithMemory()

// Make requests - automatically throttled
for i := 0; i < 100; i++ {
    ai.Ask(ctx, fmt.Sprintf("Question %d", i))
    // First 20 requests use burst capacity (immediate)
    // Remaining requests throttled to 10/second
}
```

**Per-user rate limiting for multi-tenant applications:**

```go
config := agent.RateLimitConfig{
    Enabled:           true,
    RequestsPerSecond: 5.0,
    BurstSize:         10,
    PerKey:            true,  // Independent limits per key
    KeyTimeout:        5 * time.Minute,
}

// Different users get independent rate limits
aiUser1 := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithRateLimitConfig(config).
    WithRateLimitKey("user-123")  // User 1's quota

aiUser2 := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithRateLimitConfig(config).
    WithRateLimitKey("user-456")  // User 2's quota (independent)
```

**[üìñ Rate Limiting Guide](docs/RATE_LIMITING_GUIDE.md)** - Algorithms, best practices, Redis backend

### 8. Using Ollama (Local LLM)

```go
// Simple usage - default base URL is http://localhost:11434/v1
response, err := agent.NewOllama("qwen2.5:3b").
    Ask(ctx, "What is Go?")

// With configuration
response, err := agent.NewOllama("qwen2.5:3b").
    WithSystem("You are a concise assistant").
    WithTemperature(0.8).
    WithMemory().
    Ask(ctx, "Explain goroutines")
```

### 9. ReAct Pattern - Native Function Calling (v0.7.5 üÜï)

**ReAct (Reasoning + Acting)** with **native function calling** for reliable tool execution:

```go
// Define tools
calculator := agent.NewTool("calculator", "Perform calculations").
    WithHandler(calcHandler)
search := agent.NewTool("search", "Search the web").
    WithHandler(searchHandler)

// Enable ReAct with task complexity (v0.7.6+ recommended)
ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTools(calculator, search).
    WithReActMode(true).         // Enable ReAct pattern  
    WithReActComplexity(agent.ReActTaskMedium). // Auto-configure for medium tasks

// Execute complex multi-step task
result, _ := ai.Ask(ctx, "What is 15 * 7 and what's the weather in Paris?")

// The agent autonomously:
// 1. Thought: "First I'll calculate 15 * 7"
// 2. Action: calculator("15 * 7")
// 3. Observation: "105"
// 4. Thought: "Now I need Paris weather"
// 5. Action: search("Paris weather")
// 6. Observation: "15¬∞C, Cloudy"
// 7. Answer: "15 * 7 = 105. Weather in Paris is 15¬∞C and cloudy."

// Access full reasoning trace
reactResult := result.Metadata["react_result"].(*agent.ReActResult)
for i, step := range reactResult.Steps {
    fmt.Printf("[Step %d] %s: %s\n", i+1, step.Type, step.Content)
}
```

#### Task Complexity Levels (v0.7.6+)

Choose the right complexity for better UX:

```go
// Simple tasks: 3 iterations, 30s timeout
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithReActMode(true).
    WithReActComplexity(agent.ReActTaskSimple).
    WithTools(mathTool)
// Use for: Single calculation, direct lookup, simple queries

// Medium tasks: 5 iterations, 60s timeout  
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithReActMode(true).
    WithReActComplexity(agent.ReActTaskMedium).
    WithTools(mathTool, searchTool)
// Use for: Multi-step reasoning, 2-3 tool calls, analysis

// Complex tasks: 10 iterations, 120s timeout
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithReActMode(true).
    WithReActComplexity(agent.ReActTaskComplex).
    WithTools(mathTool, searchTool, dbTool)
// Use for: Advanced reasoning, multiple tools, complex workflows
```

**Features:**

- ‚úÖ Autonomous multi-step reasoning
- ‚úÖ Tool orchestration (chains multiple tools naturally)
- ‚úÖ Auto-fallback on max iterations (v0.7.6+)
- ‚úÖ Progressive urgency reminders (v0.7.6+)
- ‚úÖ Rich error messages with debugging info (v0.7.6+)
- ‚úÖ Transparent reasoning (full trace of thoughts and actions)
- ‚úÖ Streaming support for real-time progress

**[üìñ ReAct Pattern Guide](docs/guides/REACT_GUIDE.md)** - Full documentation, best practices, and advanced features

**[üöÄ Native ReAct Examples](examples/react_native/)** - Comprehensive demos showing native function calling

**[üîß Troubleshooting](docs/REACT_TROUBLESHOOTING.md)** - Common issues and solutions (v0.7.6+)

### 9. Planning Layer - Complex Workflows (v0.7.1 üÜï)

**Goal-oriented planning** with automatic task decomposition, dependency management, and adaptive execution:

```go
// High-level API - automatic planning and execution
result, _ := agent.NewOpenAI("gpt-4o", apiKey).
    PlanAndExecute(ctx, "Research AI trends and write a comprehensive report")

// The agent autonomously:
// 1. Decomposes goal into tasks (research, analyze, synthesize, write)
// 2. Manages dependencies (can't write before research)
// 3. Executes in optimal order (parallel when possible)
// 4. Tracks progress and metrics

fmt.Printf("Completed %d tasks in %v\n",
    result.Metrics.TaskCount,
    result.Metrics.ExecutionTime)

// Advanced: Manual control with custom strategies
plan := agent.NewPlan("ETL Pipeline", agent.StrategyParallel)
plan.AddTask(agent.Task{ID: "extract-1", Description: "Extract from DB1"})
plan.AddTask(agent.Task{ID: "extract-2", Description: "Extract from DB2"})
plan.AddTask(agent.Task{
    ID:           "transform",
    Description:  "Transform combined data",
    Dependencies: []string{"extract-1", "extract-2"}, // Wait for both
})

config := agent.DefaultPlannerConfig()
config.MaxParallel = 10                    // 10 concurrent tasks
config.Strategy = agent.StrategyAdaptive   // Auto-optimize

executor := agent.NewExecutor(config, aiAgent)
result, _ := executor.Execute(ctx, plan)

// Monitor execution
for _, event := range result.Timeline {
    fmt.Printf("[%v] %s\n", event.Timestamp, event.Description)
}
```

**Features:**

- ‚úÖ **3 Execution Strategies**: Sequential, Parallel, Adaptive (auto-switching)
- ‚úÖ **Dependency Management**: Direct, transitive, diamond patterns, cycle detection
- ‚úÖ **Goal-Oriented**: Early termination when goals met
- ‚úÖ **Performance**: ~8.4¬µs topological sort, 97.6 tasks/sec throughput (parallel)
- ‚úÖ **Monitoring**: Timeline events, metrics (success rate, latency, efficiency)
- ‚úÖ **Production-Ready**: Timeout, cancellation, error recovery

**When to Use:**

| Use Case | Strategy | Example |
|----------|----------|---------|
| Simple workflow (< 5 tasks) | Sequential | Setup ‚Üí Configure ‚Üí Execute |
| Batch processing | Parallel | Process 100 items concurrently |
| Multi-phase pipeline | Adaptive | Research (parallel) ‚Üí Analyze (sequential) ‚Üí Report |
| Complex dependencies | Sequential | Long dependency chains |

**[üìñ Planning Guide](docs/PLANNING_GUIDE.md)** - Concepts, patterns, best practices  
**[üìñ Planning API](docs/PLANNING_API.md)** - Complete API reference  
**[üìñ Planning Performance](docs/PLANNING_PERFORMANCE.md)** - Benchmarks, optimization, tuning

### 10. Redis Cache - Distributed Caching (v0.5.1 üÜï)

```go
// Simple Redis cache setup
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithRedisCache("localhost:6379", "", 0)

// First call - cache miss (~1-2s)
resp1, _ := ai.Ask(ctx, "What is Go?")

// Second call - cache hit (~5ms, 200x faster!)
resp2, _ := ai.Ask(ctx, "What is Go?")

// Check cache statistics
stats := ai.GetCacheStats()
fmt.Printf("Hit rate: %.2f%%\n",
    float64(stats.Hits)/(float64(stats.Hits+stats.Misses))*100)

// Advanced configuration
opts := &agent.RedisCacheOptions{
    Addrs:       []string{"localhost:6379"},
    Password:    "your-redis-password",
    PoolSize:    20,                 // Connection pool
    KeyPrefix:   "myapp",            // Namespace
    DefaultTTL:  10 * time.Minute,   // Cache expiration
}

ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithRedisCacheOptions(opts)

// Custom TTL per request
ai.WithCacheTTL(1 * time.Hour).Ask(ctx, "Historical facts")

// Redis Cluster support
opts := &agent.RedisCacheOptions{
    Addrs: []string{
        "redis-node1:6379",
        "redis-node2:6379",
        "redis-node3:6379",
    },
    Password: "cluster-password",
}
```

**Benefits:**

- Shared cache across multiple instances
- Persistent cache (survives restarts)
- Distributed locking (prevents cache stampede)
- Scalable with Redis Cluster

### 10. Logging & Observability (v0.5.2 üÜï)

```go
// Debug logging for development
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithDebugLogging() // Detailed logs with timing, cache, tools, RAG

response, err := builder.Ask(ctx, "Hello!")
// Output:
// [2024-01-15 10:30:45] DEBUG: Ask request started | model=gpt-4o-mini
// [2024-01-15 10:30:45] DEBUG: Cache miss | duration_ms=2
// [2024-01-15 10:30:46] INFO: Ask completed | duration_ms=890 tokens=23

// Info logging for production
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithInfoLogging() // Important events only

// Slog integration (Go 1.21+)
handler := slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{
    Level: slog.LevelInfo,
})
logger := slog.New(handler)

builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithLogger(agent.NewSlogAdapter(logger))
// Output: {"time":"...","level":"INFO","msg":"Ask completed","duration_ms":890}

// Custom logger (Zap, Logrus, etc.)
type MyLogger struct { /* implement Logger interface */ }
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithLogger(&MyLogger{})

// No logging (default - zero overhead)
builder := agent.NewOpenAI("gpt-4o-mini", apiKey)
// NoopLogger - literally zero cost
```

**What gets logged:**

- Request lifecycle (start, duration, completion)
- Token usage (prompt, completion, total)
- Cache operations (hit/miss, duration)
- Tool execution (rounds, tool calls, results)
- RAG retrieval (docs retrieved, method)
- Retry attempts (delays, errors)
- Errors with context

üìñ **[Complete Logging Guide](docs/LOGGING_GUIDE.md)** - Custom loggers, slog integration, production best practices

### 11. Built-in Tools (v0.5.5 üÜï Convenient Loading)

go-deep-agent provides 4 production-ready built-in tools. **v0.5.5** introduces convenient helpers for easy loading:

#### Quick Start - Safe Tools by Default

```go
import (
    "github.com/taipm/go-deep-agent/agent"
    "github.com/taipm/go-deep-agent/agent/tools"
)

// Option 1: Load safe tools (DateTime + Math) - RECOMMENDED ‚≠ê
ai := tools.WithDefaults(agent.NewOpenAI("gpt-4o", apiKey)).
    WithAutoExecute(true)

// Option 2: Load all tools (includes file & network access) - Use with caution
ai := tools.WithAll(agent.NewOpenAI("gpt-4o", apiKey)).
    WithAutoExecute(true)

// Option 3: Manual selection (full control)
ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTools(tools.NewDateTimeTool(), tools.NewMathTool()).
    WithAutoExecute(true)
```

**Why `WithDefaults()`?** (v0.5.5)
- ‚úÖ **Safe**: DateTime and Math have no file/network access
- ‚úÖ **No side effects**: Read-only time, pure math computations
- ‚úÖ **Core capabilities**: Nearly every AI agent needs time context and math
- ‚úÖ **One-liner**: Get started quickly with sensible defaults

**Why FileSystem/HTTP remain opt-in?**
- ‚ö†Ô∏è **Powerful but risky**: Can read/write files, make external requests
- ‚ö†Ô∏è **Explicit consent**: User should know agent has these capabilities
- ‚ö†Ô∏è **Principle of least privilege**: Only grant what's needed

#### 10.1 FileSystem Tool (Opt-in for security)

#### 10.1 FileSystem Tool (Opt-in for security)

File operations with path traversal prevention.

```go
fsTool := tools.NewFileSystemTool()

ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTool(fsTool).  // Explicitly add for security awareness
    WithAutoExecute(true)

response, _ := ai.Ask(ctx, "Read config.json and list all JSON files in current directory")
```

**Operations** (7 total):
- `read_file`, `write_file`, `append_file`, `delete_file`
- `list_directory`, `file_exists`, `create_directory`
- **Security**: Path traversal prevention (`../` blocked)

#### 10.2 HTTP Tool (Opt-in for security)

Full HTTP client for API requests.

```go
httpTool := tools.NewHTTPRequestTool()

ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTool(httpTool).  // Explicitly add for security awareness
    WithAutoExecute(true)

response, _ := ai.Ask(ctx, "Fetch https://api.github.com/users/github and summarize")
```

**Features**:
- Methods: GET, POST, PUT, DELETE
- Custom headers, timeout (default 30s)
- JSON parsing

#### 10.3 DateTime Tool (Safe - auto-loadable via WithDefaults)

Date/time operations with timezone support.

```go
// Auto-loaded with WithDefaults(), or manually:
dtTool := tools.NewDateTimeTool()

ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTool(dtTool).
    WithAutoExecute(true)

response, _ := ai.Ask(ctx, "What day of the week is Christmas 2025 in Tokyo timezone?")
```

**Operations** (7 total):
- `current_time`, `format_date`, `parse_date`
- `add_duration`, `date_diff`, `convert_timezone`, `day_of_week`
- **Timezones**: UTC, America/New_York, Asia/Tokyo, etc.
- **Safe**: Read-only, no side effects

#### 10.4 Math Tool (Safe - auto-loadable via WithDefaults)

Professional-grade math powered by **govaluate** + **gonum**.

#### 10.4 Math Tool (Safe - auto-loadable via WithDefaults)

Professional-grade math powered by **govaluate** + **gonum**.

```go
// Auto-loaded with WithDefaults(), or manually:
mathTool := tools.NewMathTool()

ai := agent.NewOpenAI("gpt-4o", apiKey).
    WithTool(mathTool).
    WithAutoExecute(true)

// Expression evaluation (11 functions)
ai.Ask(ctx, "Calculate: 2 * (3 + 4) + sqrt(16)")
// Functions: sqrt, pow, sin, cos, tan, log, ln, abs, ceil, floor, round

// Statistics (gonum/stat)
ai.Ask(ctx, "What's the average of 10, 20, 30, 40, 50?")
// Measures: mean, median, stdev, variance, min, max, sum

// Equation solving
ai.Ask(ctx, "Solve: x+15=42")

// Unit conversion
ai.Ask(ctx, "Convert 100 km to meters")

// Random generation
ai.Ask(ctx, "Pick a random number from 1 to 100")
```

**Operations** (5 categories):
- **evaluate**: Expressions with 11 functions (govaluate engine)
- **statistics**: 7 measures (gonum library)
- **solve**: Linear equations (quadratic coming v0.6.0)
- **convert**: Distance, weight, temperature, time
- **random**: Integer, float, choice

**Safe**: Pure computations, no I/O operations  
**Dependencies**: +9MB binary for professional accuracy

üìñ **[View builtin_tools_demo.go](examples/builtin_tools_demo.go)** - Complete examples  
üìñ **[View test_with_defaults.go](examples/test_with_defaults.go)** - v0.5.5 WithDefaults() usage

### 12. History Management

```go
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).WithMemory()

builder.Ask(ctx, "I love Go programming")
builder.Ask(ctx, "What are channels?")

// Get conversation history
history := builder.GetHistory()
fmt.Printf("Messages: %d\n", len(history))

// Clear conversation (keeps system prompt)
builder.Clear()

// Save and restore sessions
savedHistory := builder.GetHistory()
// ... later ...
builder.SetHistory(savedHistory)
```

### 13. Multimodal - Vision (GPT-4 Vision)

```go
// Analyze image from URL
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithImage("https://example.com/photo.jpg").
    Ask(ctx, "What do you see in this image?")

// Compare multiple images with detail control
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithImageURL("https://example.com/image1.jpg", agent.ImageDetailLow).
    WithImageURL("https://example.com/image2.jpg", agent.ImageDetailHigh).
    Ask(ctx, "Compare these two images")

// Analyze local image file
response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithImageFile("./chart.png", agent.ImageDetailHigh).
    Ask(ctx, "Extract data from this chart")

// Conversation with images
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).WithMemory()
builder.WithImage("https://example.com/photo.jpg").
    Ask(ctx, "What's in this image?")
builder.Ask(ctx, "What colors are prominent?") // Remembers the image
```

### 14. Vector RAG - Semantic Search (v0.5.0 üÜï)

```go
// Setup vector database and embeddings
embedding, _ := agent.NewOllamaEmbedding(
    "http://localhost:11434",
    "nomic-embed-text", // Free local embeddings
)

store, _ := agent.NewChromaStore("http://localhost:8000")
store.WithEmbedding(embedding)

// Create collection
config := &agent.CollectionConfig{
    Name:           "company-kb",
    Dimension:      768,
    DistanceMetric: agent.DistanceMetricCosine,
}
store.CreateCollection(ctx, "company-kb", config)

// Create agent with vector RAG
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithVectorRAG(embedding, store, "company-kb").
    WithRAGTopK(3).         // Retrieve top 3 similar docs
    WithMemory()

// Add knowledge base
docs := []string{
    "Our refund policy allows full refunds within 30 days.",
    "Customer support is available 24/7 at support@company.com.",
    "We support integrations with Slack, Teams, and Salesforce.",
}
ai.AddDocumentsToVector(ctx, docs...)

// Ask questions - semantically retrieves relevant context
response, _ := ai.Ask(ctx, "What is your refund policy?")
fmt.Println(response)

// See what was retrieved
retrieved := ai.GetLastRetrievedDocs()
for _, doc := range retrieved {
    fmt.Printf("Score: %.3f | %s\n", doc.Score, doc.Content)
}
```

### 15. Advanced Vector RAG with Metadata

```go
// Add documents with rich metadata
vectorDocs := []*agent.VectorDocument{
    {
        Content: "Python is great for data science and machine learning.",
        Metadata: map[string]interface{}{
            "category":   "programming",
            "language":   "Python",
            "difficulty": "beginner",
            "tags":       []string{"data-science", "ml"},
        },
    },
    {
        Content: "Go is excellent for building high-performance backend services.",
        Metadata: map[string]interface{}{
            "category":   "programming",
            "language":   "Go",
            "difficulty": "intermediate",
            "tags":       []string{"backend", "concurrency"},
        },
    },
}

ai.AddVectorDocuments(ctx, vectorDocs...)

// Query with custom config
ai := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithVectorRAG(embedding, store, "docs").
    WithRAGConfig(&agent.RAGConfig{
        TopK:          5,     // Retrieve top 5
        MinScore:      0.7,   // Only high-confidence results
        IncludeScores: true,  // Show relevance scores
    })

response, _ := ai.Ask(ctx, "Tell me about backend programming")

// Access retrieved metadata
docs := ai.GetLastRetrievedDocs()
for _, doc := range docs {
    fmt.Printf("Language: %s, Difficulty: %s\n",
        doc.Metadata["language"],
        doc.Metadata["difficulty"])
}
```

### 16. Switch Vector Databases - ChromaDB vs Qdrant

```go
// Development: Use ChromaDB (easy setup)
chromaStore, _ := agent.NewChromaStore("http://localhost:8000")
aiDev := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithVectorRAG(embedding, chromaStore, "docs")

// Production: Use Qdrant (high performance)
qdrantStore, _ := agent.NewQdrantStore("http://localhost:6333")
qdrantStore.WithAPIKey("your-api-key") // Optional
aiProd := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithVectorRAG(embedding, qdrantStore, "docs")

// Both use the same API - seamless switching!
```

## üìñ Builder API Methods

### Core Methods

- `NewOpenAI(model, apiKey)` - Create OpenAI builder
- `NewOllama(model)` - Create Ollama builder (localhost:11434)
- `New(provider, model)` - Generic constructor
- `Ask(ctx, message)` - Send message, get response
- `Stream(ctx, message)` - Stream response with callbacks
- `StreamPrint(ctx, message)` - Stream and print to stdout

### Configuration

- `WithAPIKey(key)` - Set API key
- `WithBaseURL(url)` - Custom endpoint
- `WithModel(model)` - Change model
- `WithSystem(prompt)` - System prompt
- `WithTemperature(temp)` - Sampling temperature (0-2)
- `WithTopP(topP)` - Nucleus sampling (0-1)
- `WithMaxTokens(max)` - Maximum tokens to generate
- `WithPresencePenalty(penalty)` - Presence penalty (-2 to 2)
- `WithFrequencyPenalty(penalty)` - Frequency penalty (-2 to 2)
- `WithSeed(seed)` - For reproducible outputs
- `WithN(n)` - Number of completions to generate

### Conversation Management

- `WithMemory()` - Enable automatic conversation memory
- `WithMaxHistory(max)` - Limit messages (FIFO truncation)
- `WithEpisodicMemory(threshold)` - Enable episodic storage (0.0-1.0)
- `WithWorkingMemorySize(size)` - Set working memory capacity
- `WithImportanceWeights(weights)` - Customize importance calculation
- `WithSemanticMemory()` - Enable fact storage
- `GetMemory()` - Access memory system for advanced operations
- `DisableMemory()` - Disable hierarchical memory (use simple FIFO)
- `GetHistory()` - Get conversation messages
- `SetHistory(messages)` - Restore conversation
- `Clear()` - Reset conversation (keeps system prompt)

### Tool Calling

- `WithTools(tools...)` - Register tools/functions
- `WithAutoExecute(enable)` - Auto-execute tool calls
- `WithMaxToolRounds(max)` - Max execution rounds (default 5)
- `WithToolChoice(choice)` - Control when LLM uses tools (v0.7.8 üÜï)
  - `"auto"` - Let LLM decide (default)
  - `"required"` - Force tool usage (compliance, audit trails)
  - `"none"` - Disable tools temporarily
- `OnToolCall(callback)` - Tool call callback

**Tool Choice Control** (v0.7.8) - Fine-grained control over tool usage:

```go
// REQUIRED mode: Force tool usage for compliance
builder := agent.NewOpenAI("gpt-4o", apiKey).
    WithTools(calculatorTool).
    WithAutoExecute(true).
    WithToolChoice("required").  // Guarantee tool execution
    Ask(ctx, "Calculate total: 1000 shares at $750.50")
// ‚úì Calculation verified via tool - audit trail available
```

Use cases:
- **Compliance**: Financial calculations, legal verification, healthcare data (auditable)
- **Quality Control**: Guarantee 100% accurate data via tool verification
- **API Integration**: Force real-time data retrieval, prevent hallucination
- **Security**: Mandatory verification steps
- **Testing**: `"none"` mode to test LLM reasoning without tools

### Multimodal Support (Vision)

- `WithImage(url)` - Add image with auto detail level
- `WithImageURL(url, detail)` - Add image with specific detail (Low/High)
- `WithImageFile(filePath, detail)` - Add local image file
- `WithImageBase64(base64Data, mimeType, detail)` - Add base64-encoded image
- `ClearImages()` - Remove pending images

Supported models: `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-4-vision-preview`

### Structured Outputs

- `WithJSONMode()` - Force JSON output
- `WithJSONSchema(name, desc, schema, strict)` - Structured JSON

### Streaming Callbacks

- `OnStream(callback)` - Content chunk callback
- `OnRefusal(callback)` - Refusal detection callback

### Error Handling & Recovery

- `WithTimeout(duration)` - Request timeout
- `WithRetry(maxRetries)` - Retry failed requests
- `WithRetryDelay(delay)` - Fixed delay between retries
- `WithExponentialBackoff()` - Use exponential backoff

### Error Codes & Debugging (v0.5.9 üÜï)

**Error Codes** for programmatic decisions:

- `GetErrorCode(err)` - Extract error code from any error
- `IsCodedError(err)` - Check if error has a code
- `NewCodedError(code, msg, err)` - Create coded error
- **20+ error codes** including: `ErrCodeRateLimitExceeded`, `ErrCodeRequestTimeout`, `ErrCodeAPIKeyMissing`, etc.

**Debug Mode** for visibility:

- `WithDebug(config)` - Enable debug logging with secret redaction
- `DefaultDebugConfig()` - Basic logging (production-safe)
- `VerboseDebugConfig()` - Full logging (development)
- **Auto secret redaction**: API keys, tokens, passwords automatically masked

**Panic Recovery** for stability:

- `IsPanicError(err)` - Check if error is from panic
- `GetPanicValue(err)` - Extract panic value
- `GetStackTrace(err)` - Get full stack trace
- **Automatic recovery**: Tool panics are caught and returned as errors

**Error Context** for debugging:

- `WithContext(err, operation, details)` - Add context to errors
- `WithSimpleContext(err, operation)` - Quick context without details
- `SummarizeError(err)` - Get comprehensive error summary
- `NewErrorChain()` - Track multiple errors in workflows

See [ERROR_HANDLING_BEST_PRACTICES.md](docs/ERROR_HANDLING_BEST_PRACTICES.md) for complete guide.

### Error Type Checking

- `IsAPIKeyError(err)` - Check for API key errors
- `IsRateLimitError(err)` - Check for rate limits
- `IsTimeoutError(err)` - Check for timeouts
- `IsRefusalError(err)` - Check for content refusals
- `IsInvalidResponseError(err)` - Check for invalid responses
- `IsMaxRetriesError(err)` - Check if retries exhausted
- `IsToolExecutionError(err)` - Check for tool errors

### Vector RAG (v0.5.0 üÜï)

- `WithVectorRAG(embedding, store, collection)` - Enable vector-based RAG
- `AddDocumentsToVector(ctx, docs...)` - Add string documents to vector store
- `AddVectorDocuments(ctx, vectorDocs...)` - Add documents with metadata
- `GetLastRetrievedDocs()` - Get retrieved documents with scores

### Response Caching (v0.4.0, v0.5.1 üÜï)

- `WithCache(cache)` - Set custom cache implementation
- `WithMemoryCache(maxSize, defaultTTL)` - In-memory LRU cache
- `WithRedisCache(addr, password, db)` - Redis distributed cache (simple)
- `WithRedisCacheOptions(opts)` - Redis cache with advanced config
- `WithCacheTTL(ttl)` - Set custom TTL for next request
- `DisableCache()` - Temporarily disable caching
- `EnableCache()` - Re-enable caching
- `GetCacheStats()` - Retrieve cache statistics (hits, misses, hit rate)
- `ClearCache(ctx)` - Clear all cached responses

### Embedding Providers (v0.5.0 üÜï)

- `NewOllamaEmbedding(baseURL, model)` - Free local embeddings (Ollama)
- `NewOpenAIEmbedding(apiKey, model, dimension)` - OpenAI embeddings
- `Generate(ctx, texts)` - Generate embeddings for texts
- `GenerateQuery(ctx, query)` - Generate embedding for search query

### Vector Stores (v0.5.0 üÜï)

- `NewChromaStore(baseURL)` - Create ChromaDB client
- `NewQdrantStore(baseURL)` - Create Qdrant client
- `CreateCollection(ctx, name, config)` - Create collection with config
- `Add(ctx, collection, documents)` - Add documents with auto-embedding
- `Search(ctx, request)` - Vector similarity search
- `SearchByText(ctx, request)` - Text-based semantic search
- `Delete(ctx, collection, ids)` - Delete documents by IDs
- `Count(ctx, collection)` - Get document count
- `Clear(ctx, collection)` - Remove all documents

## üèóÔ∏è Project Structure

```plaintext
go-deep-agent/
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îú‚îÄ‚îÄ builder.go              # Fluent Builder API
‚îÇ   ‚îú‚îÄ‚îÄ errors.go               # Custom error types
‚îÇ   ‚îú‚îÄ‚îÄ tools.go                # Tool calling support
‚îÇ   ‚îú‚îÄ‚îÄ *_test.go               # Comprehensive tests (76 tests)
‚îÇ   ‚îî‚îÄ‚îÄ README.md               # API documentation
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ builder_basic.go        # Basic examples
‚îÇ   ‚îú‚îÄ‚îÄ builder_streaming.go   # Streaming examples
‚îÇ   ‚îú‚îÄ‚îÄ builder_tools.go        # Tool calling examples
‚îÇ   ‚îú‚îÄ‚îÄ builder_json_schema.go # JSON Schema examples
‚îÇ   ‚îú‚îÄ‚îÄ builder_conversation.go # Memory management
‚îÇ   ‚îú‚îÄ‚îÄ builder_errors.go       # Error handling
‚îÇ   ‚îú‚îÄ‚îÄ ollama_example.go       # Ollama examples
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ main.go                     # Quick start demo
‚îú‚îÄ‚îÄ README.md                   # You are here
‚îú‚îÄ‚îÄ BUILDER_API.md              # Complete Builder API guide
‚îú‚îÄ‚îÄ TODO.md                     # Development roadmap
‚îî‚îÄ‚îÄ go.mod
```

## üìä Quality Metrics

- ‚úÖ **460+ Tests** passing across all features
- ‚úÖ **65%+ Coverage** with comprehensive test cases
- ‚úÖ **15 Example Files** with 70+ working examples
- ‚úÖ **Production Libraries** (openai-go, govaluate, gonum)
- ‚úÖ **Production Tested** with real OpenAI, Ollama, ChromaDB, Qdrant

## üõ†Ô∏è Setup & Usage

### OpenAI Setup

```bash
# Set your API key
export OPENAI_API_KEY=your-api-key-here

# Run examples
go run main.go
```

### Ollama Setup

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull qwen2.5:3b

# Pull embedding model (for vector RAG)
ollama pull nomic-embed-text

# Run Ollama server
ollama serve

# Run Ollama examples
go run examples/ollama_example.go
```

### Vector Database Setup (v0.5.0 üÜï)

```bash
# ChromaDB (easiest for development)
docker run -p 8000:8000 chromadb/chroma

# OR Qdrant (production-ready)
docker run -p 6333:6333 qdrant/qdrant

# Run vector RAG examples
go run examples/vector_rag_example.go
go run examples/chroma_example.go
go run examples/qdrant_example.go
```

## üèÉ Running Examples

```bash
# Basic examples
go run examples/builder_basic.go

# Streaming examples
go run examples/builder_streaming.go

# Tool calling examples
go run examples/builder_tools.go

# JSON Schema examples (requires OPENAI_API_KEY)
go run examples/builder_json_schema.go

# Conversation management
go run examples/builder_conversation.go

# Error handling examples
go run examples/builder_errors.go

# Ollama examples (requires Ollama running)
go run examples/ollama_example.go

# Planning Layer examples (v0.7.1 üÜï)
go run examples/planner_basic/main.go       # Basic sequential planning
go run examples/planner_parallel/main.go    # Parallel batch processing
go run examples/planner_adaptive/main.go    # Adaptive strategy switching

# Vector RAG examples (v0.5.0 üÜï)
go run examples/embedding_example.go      # Embedding basics
go run examples/chroma_example.go         # ChromaDB integration
go run examples/qdrant_example.go         # Qdrant integration
go run examples/vector_rag_example.go     # Complete RAG workflow

# Quick demo with all features
go run main.go
```

## üéØ Design Philosophy

1. **Fluent API** - Method chaining for natural, readable code
2. **Smart Defaults** - Works out of the box, customize as needed
3. **Memory Management** - Automatic conversation history with FIFO truncation
4. **Error Recovery** - Intelligent retries with exponential backoff
5. **Type Safety** - Leverages Go's type system for safety
6. **Zero Surprises** - Predictable behavior, no hidden magic
7. **Production Ready** - Timeouts, retries, comprehensive error handling

## üß© Advanced Use Cases

### Multi-Round Tool Execution

```go
calculateTool := agent.NewTool("calculate", "Perform math calculations").
    AddParameter("expression", "string", "Math expression", true).
    WithHandler(func(args string) (string, error) {
        // ... calculation logic ...
        return result, nil
    })

response, err := agent.NewOpenAI("gpt-4o-mini", apiKey).
    WithTools(calculateTool).
    WithAutoExecute(true).
    WithMaxToolRounds(5).  // Allow multiple tool calls
    Ask(ctx, "Calculate (10 + 20) * 3, then add 50")
```

### Session Persistence

```go
builder := agent.NewOpenAI("gpt-4o-mini", apiKey).WithMemory()

// Have conversation
builder.Ask(ctx, "I'm learning Go")
builder.Ask(ctx, "Tell me about channels")

// Save session
session := builder.GetHistory()
saveToDatabase(session)

// Later: restore session
loadedSession := loadFromDatabase()
builder.SetHistory(loadedSession)
builder.Ask(ctx, "What were we talking about?")
```

### Production Error Handling

```go
func robustAsk(ctx context.Context, prompt string) (string, error) {
    builder := agent.NewOpenAI("gpt-4o-mini", apiKey).
        WithTimeout(30 * time.Second).
        WithRetry(3).
        WithExponentialBackoff()

    response, err := builder.Ask(ctx, prompt)
    if err != nil {
        if agent.IsTimeoutError(err) {
            return "", fmt.Errorf("request timed out after 30s")
        }
        if agent.IsRateLimitError(err) {
            time.Sleep(60 * time.Second) // Wait and retry
            return robustAsk(ctx, prompt)
        }
        return "", err
    }
    return response, nil
}
```

## üìã Requirements

- **Go 1.23.3** or higher
- **OpenAI API key** (for OpenAI provider)
- **Ollama** running locally (for Ollama provider)

## üÜö Why Choose go-deep-agent?

**60-80% less code** than raw openai-go SDK with **10x better developer experience**.

| Feature | openai-go | go-deep-agent | Improvement |
|---------|-----------|---------------|-------------|
| Simple Chat | 26 lines | 14 lines | ‚¨áÔ∏è 46% |
| Streaming | 20+ lines | 5 lines | ‚¨áÔ∏è 75% |
| Memory | 28+ lines (manual) | 6 lines (auto) | ‚¨áÔ∏è 78% |
| Tool Calling | 50+ lines | 14 lines | ‚¨áÔ∏è 72% |
| Multimodal | 25+ lines | 5 lines | ‚¨áÔ∏è 80% |

**[üìñ See detailed comparison with code examples ‚Üí](docs/COMPARISON.md)**

### Key Advantages

- ‚úÖ **Fluent API** - Method chaining reads like natural language
- ‚úÖ **Modular Architecture** - Builder split into 10 focused files (61% reduction, v0.6.0 refactoring)
- ‚úÖ **Automatic Features** - Memory, retry, error handling built-in
- ‚úÖ **Production-Ready** - 470+ tests, 65.2% coverage, CI/CD, comprehensive benchmarks
- ‚úÖ **Better DX** - IDE autocomplete, self-documenting code, clear file organization
- ‚úÖ **All openai-go Features** - Plus high-level conveniences

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Ensure all tests pass: `go test ./...`
5. Submit a pull request

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details

## ÔøΩ Security

**Overall Grade: B+ (82/100)** - Good security foundation with areas for improvement

### Current Security Features

‚úÖ **Input Validation** - 30+ Validate() methods across configs  
‚úÖ **Secret Redaction** - 6 regex patterns for API keys/tokens in debug logs  
‚úÖ **Path Traversal Prevention** - Blocks ".." in filesystem tool  
‚úÖ **Timeout Protection** - 30s default for HTTP/tools/requests  
‚úÖ **Structured Error Handling** - Security context tracking  

### Security Best Practices

```go
// ‚úÖ Use environment variables for API keys
apiKey := os.Getenv("OPENAI_API_KEY")
if apiKey == "" {
    log.Fatal("OPENAI_API_KEY not set")
}

// ‚úÖ Only enable safe tools
ai := tools.WithDefaults(agent.NewOpenAI("gpt-4o", apiKey))

// ‚úÖ Set timeouts and retries
ai := ai.WithTimeout(30 * time.Second).WithMaxRetries(3)

// ‚úÖ Disable debug in production
debug := os.Getenv("ENV") != "production"
ai := ai.WithDebug(debug)
```

### Security Documentation

- **[SECURITY_SUMMARY.md](docs/SECURITY_SUMMARY.md)** - üîí Quick reference for security (v0.5.9 üÜï)
- **[SECURITY_ANALYSIS.md](docs/SECURITY_ANALYSIS.md)** - üîí Comprehensive security assessment (v0.5.9 üÜï)

## ÔøΩüìö Documentation

- **[README.md](README.md)** - Main documentation (you are here)
- **[MEMORY_SYSTEM_GUIDE.md](docs/MEMORY_SYSTEM_GUIDE.md)** - üß† Complete memory system guide: Memory vs Cache vs Vector Store (v0.7.10 üÜï)
- **[COMPARISON.md](docs/COMPARISON.md)** - üÜö Why go-deep-agent vs openai-go (with code examples)
- **[FEWSHOT_GUIDE.md](docs/FEWSHOT_GUIDE.md)** - üéì Few-Shot Learning complete guide (v0.6.5)
- **[PLANNING_GUIDE.md](docs/PLANNING_GUIDE.md)** - üß© Planning Layer concepts and patterns (v0.7.1 üÜï)
- **[PLANNING_API.md](docs/PLANNING_API.md)** - üß© Planning Layer API reference (v0.7.1 üÜï)
- **[PLANNING_PERFORMANCE.md](docs/PLANNING_PERFORMANCE.md)** - üß© Planning Layer benchmarks and tuning (v0.7.1 üÜï)
- **[CHANGELOG.md](CHANGELOG.md)** - Version history and migration guides
- **[ERROR_HANDLING_BEST_PRACTICES.md](docs/ERROR_HANDLING_BEST_PRACTICES.md)** - üÜï Complete error handling guide (v0.5.9)
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - üÜï Common issues and solutions (v0.5.9)
- **[BUILDER_REFACTORING_PROPOSAL.md](docs/BUILDER_REFACTORING_PROPOSAL.md)** - üÜï Builder refactoring details (v0.6.0)
- **[RAG_VECTOR_DATABASES.md](docs/RAG_VECTOR_DATABASES.md)** - Complete Vector RAG guide (v0.5.0)
- **[LOGGING_GUIDE.md](docs/LOGGING_GUIDE.md)** - Comprehensive logging & observability guide (v0.5.2)
- **[examples/](examples/)** - 75+ working examples across 25+ files
- **[agent/README.md](agent/README.md)** - Detailed API documentation
- **[TODO.md](TODO.md)** - Roadmap and implementation progress
- **[ROADMAP.md](ROADMAP.md)** - v0.5.0 Advanced RAG implementation plan

## üîó Links

- **GitHub**: [github.com/taipm/go-deep-agent](https://github.com/taipm/go-deep-agent)
- **openai-go**: [github.com/openai/openai-go](https://github.com/openai/openai-go) - Official OpenAI Go library
- **Ollama**: [ollama.com](https://ollama.com) - Run LLMs locally

---

<div align="center">

**Made with ‚ù§Ô∏è for the Go community**

‚≠ê Star us on GitHub if you find this useful!

</div>